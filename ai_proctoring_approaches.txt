AI Proctoring Approaches with Azure AI / Azure OpenAI
====================================================

1. Face Detection / No-face Alerts
----------------------------------
**Approach:**
- Use Azure Face API (cloud) for robust face detection.
- Optionally, use browser-based models (e.g., face-api.js, MediaPipe) for real-time detection without sending video to the cloud.

**Azure Face API Example:**
- Capture video frames (e.g., every 1-2 seconds) from the student's webcam.
- Send frame as JPEG/PNG to Azure Face API `/detect` endpoint.
- If no face detected, trigger a "No-face" alert.

```js
// Pseudocode for sending a frame to Azure Face API
const response = await fetch('https://<region>.api.cognitive.microsoft.com/face/v1.0/detect', {
  method: 'POST',
  headers: {
    'Ocp-Apim-Subscription-Key': '<your-key>',
    'Content-Type': 'application/octet-stream',
  },
  body: imageBlob, // JPEG/PNG blob from canvas
});
const faces = await response.json();
if (faces.length === 0) alert('No face detected!');
```

**Browser-based (face-api.js):**
- Use `faceapi.detectAllFaces(videoElement)` in a loop.
- If no faces, show alert.

2. Multi-face Alert
-------------------
**Approach:**
- Use the same face detection as above.
- If `faces.length > 1`, trigger a multi-face alert.

**Code:**
```js
if (faces.length > 1) alert('Multiple faces detected!');
```

3. Yawning, Looking Away
------------------------
**Approach:**
- Use Azure Face API with `faceAttributes` (e.g., headPose, mouthOpen if available).
- Or, use face-api.js/MediaPipe for head pose estimation and mouth aspect ratio.

**Azure Face API:**
- Request `returnFaceAttributes=headPose`.
- If `headPose.yaw` or `headPose.pitch` exceeds threshold, alert for looking away.
- For yawning, check if mouth is open (if supported by API).

**Code:**
```js
if (face.faceAttributes.headPose.yaw > 20 || face.faceAttributes.headPose.pitch > 20) {
  alert('Looking away detected!');
}
// For yawning, use mouth landmarks or attributes if available
```

**Browser-based:**
- Use face landmarks to compute mouth aspect ratio (MAR) for yawning.
- Use nose/eyes position for head pose.

4. Mobile Phone Detection
------------------------
**Approach:**
- Azure Custom Vision: Train a model to detect mobile phones in webcam frames.
- Or, use YOLO/MediaPipe Objectron in-browser for object detection.

**Azure Custom Vision Example:**
- Upload frames to a Custom Vision endpoint.
- If prediction includes 'mobile phone' with high confidence, alert.

**Code:**
```js
// Pseudocode for sending to Custom Vision
const response = await fetch('<custom-vision-endpoint>', {
  method: 'POST',
  headers: { 'Prediction-Key': '<your-key>', 'Content-Type': 'application/octet-stream' },
  body: imageBlob,
});
const result = await response.json();
if (result.predictions.some(p => p.tagName === 'mobile phone' && p.probability > 0.7)) {
  alert('Mobile phone detected!');
}
```

**Browser-based:**
- Use TensorFlow.js/YOLO or MediaPipe Objectron for real-time detection.

--------------------------------------------------

**General Notes:**
- For privacy, prefer browser-based detection if possible.
- For higher accuracy, use Azure cloud APIs.
- Always throttle frame sending (e.g., 1-2 fps) to avoid bandwidth/cost issues.
- Combine both approaches for best results (local detection, cloud verification).

**References:**
- Azure Face API: https://learn.microsoft.com/en-us/azure/cognitive-services/face/
- Azure Custom Vision: https://learn.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/
- face-api.js: https://github.com/justadudewhohacks/face-api.js/
- MediaPipe: https://google.github.io/mediapipe/solutions/face_mesh.html 